{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUa3/tMdObxpJYfmIVz5EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_network_one_file_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PipQ4HNTjKW",
        "colab_type": "code",
        "outputId": "f3daec44-c623-4d52-9008-e5232e7f60b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0PZxCvmWCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import networkx as nx\n",
        "from shapely.geometry import Point\n",
        "import imageio\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tMnwAEmqpF",
        "colab_type": "text"
      },
      "source": [
        "Set up the path  to data and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tLiFKrmZDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"G:/Sync/FFE/Mesa\"\n",
        "path_output = \"G:\\Sync\\FFE\\FireNetwork\"\n",
        "\n",
        "\n",
        "# path = '/Users/alex/Google Drive/05_Sync/FFE/Mesa'\n",
        "# path_output = '/Users/alex/Google Drive/05_Sync/FFE/Mesa/output'\n",
        "\n",
        "# path = '/Users/alex/Google Drive/05_Sync/FFE/Mesa'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kl25fSmjGN",
        "colab_type": "text"
      },
      "source": [
        "Create the functions to be used by the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZosFepFncaX",
        "colab": {}
      },
      "source": [
        "def load_data(file_name, minx, miny, maxx, maxy):\n",
        "    # crop data\n",
        "    bbox = box(minx, miny, maxx, maxy)\n",
        "    # building point dataset\n",
        "    gdf_buildings = gpd.read_file(os.path.join(path, file_name), bbox=bbox)\n",
        "    # gdf_buildings.IgnProb_bl = 0.02\n",
        "    # xmin,ymin,xmax,ymax = gdf_buildings.total_bounds\n",
        "    return gdf_buildings\n",
        "\n",
        "\n",
        "def wind_scenario():\n",
        "    wind_data = pd.read_csv(os.path.join(path, 'GD_wind.csv'))\n",
        "    i = np.random.randint(0, wind_data.shape[0])\n",
        "    w = wind_data.iloc[i, 2]\n",
        "    d = wind_data.iloc[i, 1]\n",
        "    b = wind_data.iloc[i, 3]\n",
        "    return w, d, b\n",
        "\n",
        "\n",
        "def eudistance(v1, v2):\n",
        "    return np.linalg.norm(v1 - v2)\n",
        "\n",
        "\n",
        "def calculate_azimuth(x1, y1, x2, y2):\n",
        "    azimuth = math.degrees(math.atan2((x2 - x1), (y2 - y1)))\n",
        "    return 360 + azimuth\n",
        "\n",
        "\n",
        "def plot(df, column_df):\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    df.plot(column=column_df, ax=ax, legend=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def build_edge_list(geodataframe, maximum_distance, polygon_file):\n",
        "    # create arrays for different id combination\n",
        "    n = np.arange(0, len(geodataframe))\n",
        "    target = [n] * len(geodataframe)\n",
        "    target = np.hstack(target)\n",
        "    source = np.repeat(n, len(geodataframe))\n",
        "    # put arrays in dataframe\n",
        "    df = pd.DataFrame()\n",
        "    df['source_id'] = source\n",
        "    df['target_id'] = target\n",
        "    # merge source attributes with source index\n",
        "    geo_df = geodataframe.copy()\n",
        "    geo_df['id'] = geo_df.index\n",
        "    # create source / target gdf from gdf.columns of interest\n",
        "    geo_df = geo_df[['id', 'TARGET_FID', 'X', 'Y', 'geometry', 'IgnProb_bl']]\n",
        "    geo_df_TRG = geo_df.copy()\n",
        "    geo_df_TRG.columns = ['target_' + str(col) for col in geo_df_TRG.columns]\n",
        "    geo_df_SRC = geo_df.copy()\n",
        "    geo_df_SRC.columns = ['source_' + str(col) for col in geo_df_SRC.columns]\n",
        "    # merge data\n",
        "    merged_data = pd.merge(df, geo_df_SRC, left_on='source_id', right_on='source_id', how='outer')\n",
        "    merged_data = pd.merge(merged_data, geo_df_TRG, left_on='target_id', right_on='target_id', how='outer')\n",
        "    merged_data.rename(columns={'source_id': 'source', 'target_id': 'target'}, inplace=True)\n",
        "    # calculate distance for each source / target pair\n",
        "    # create a df from polygon shape to get accurate distance\n",
        "    # print(list(polygon_file))\n",
        "    polygon = polygon_file[['TARGET_FID', 'geometry']]\n",
        "    # print(list(polygon))\n",
        "    source_poly = merged_data[['source_TARGET_FID']]\n",
        "    target_poly = merged_data[['target_TARGET_FID']]\n",
        "    # print(list(source_poly))\n",
        "    src_poly = pd.merge(source_poly, polygon, left_on='source_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    trg_poly = pd.merge(target_poly, polygon, left_on='target_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    src_poly_gdf = gpd.GeoDataFrame(src_poly, geometry='geometry')\n",
        "    trg_poly_gdf = gpd.GeoDataFrame(trg_poly, geometry='geometry')\n",
        "    distance_series = src_poly_gdf.distance(trg_poly_gdf)\n",
        "    # print(distance_series)\n",
        "\n",
        "    # insert distance in merged data column\n",
        "    merged_data['v1'] = merged_data.source_X - merged_data.target_X\n",
        "    merged_data['v2'] = merged_data.source_Y - merged_data.target_Y\n",
        "    # merged_data['euc_distance'] = np.hypot(merged_data.v1, merged_data.v2)\n",
        "    merged_data['euc_distance'] = distance_series\n",
        "    # remove when distance \"illegal\"\n",
        "    valid_distance = merged_data['euc_distance'] < maximum_distance\n",
        "    not_same_node = merged_data['euc_distance'] != 0\n",
        "    data = merged_data[valid_distance & not_same_node]\n",
        "    # calculate azimuth\n",
        "    data['azimuth'] = np.degrees(np.arctan2(merged_data['v2'], merged_data['v1']))\n",
        "    data['bearing'] = (data.azimuth + 360) % 360\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_network(edge_list_dataframe):\n",
        "    graph = nx.from_pandas_edgelist(edge_list_dataframe, edge_attr=True)\n",
        "    # options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "    #            'with_labels': False, 'font_weight': 'bold'}\n",
        "    # nx.draw_kamada_kawai(graph, **options)\n",
        "    # plt.show()\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjmAYO9UnNOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_initial_fire_to(df):\n",
        "    \"\"\"Fine = 0, Fire = 1, Burned = 2\"\"\"\n",
        "    df['RNG'] = np.random.uniform(0, 1, size=len(df))  # add for random suppression per building, df.shape[0])\n",
        "    onFire = df['source_IgnProb_bl'] > df['RNG']\n",
        "    ignitions = df[onFire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(ignitions.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def set_fire_to(df, existing_fires):\n",
        "    are_set_on_fire = (df['source'].isin(existing_fires))\n",
        "    spark = df[are_set_on_fire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(spark.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def fire_spreading(list_fires, list_burn, wind_speed, wind_bearing, suppression_threshold, step_value, data):\n",
        "    # check the fire potential targets\n",
        "    # print(\"fire list before spreading : {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "    are_potential_targets = (data['source'].isin(list_fires))\n",
        "    are_not_already_burned = (~data['target'].isin(list_burn))\n",
        "    df = data[are_potential_targets & are_not_already_burned]\n",
        "    if df.empty:\n",
        "        # print(\"no fires\")\n",
        "        list_burn.extend(list(list_fires))\n",
        "        list_burn = list(dict.fromkeys(list_burn))\n",
        "        return [], list_burn  # to break the step loop\n",
        "    # set up additional CONDITIONS for fire spreading\n",
        "\n",
        "    # neighbors selection from buffer\n",
        "    df['buffer_geometry'] = gdf.geometry.buffer(gdf['d_long'] + wind_speed)\n",
        "\n",
        "    are_neighbors = df['euc_distance'] < wind_speed\n",
        "    # print(\"neighbors affected ? {}\".format(list(dict.fromkeys(list(are_neighbors)))))\n",
        "    df = df[are_neighbors]\n",
        "    # wind direction\n",
        "    wind_bearing_max = wind_bearing + 45\n",
        "    wind_bearing_min = wind_bearing - 45\n",
        "    if wind_bearing == 360:\n",
        "        wind_bearing_max = 45\n",
        "    if wind_bearing <= 0:  # should not be necessary\n",
        "        wind_bearing_min = 0\n",
        "    if wind_bearing == 999:\n",
        "        wind_bearing_max = 999\n",
        "        wind_bearing_min = 0\n",
        "    are_under_the_wind = (df['bearing'] < wind_bearing_max) & (df['bearing'] > wind_bearing_min)\n",
        "    # print(\"targets under the wind ? {}\".format(list(dict.fromkeys(list(are_under_the_wind)))))\n",
        "    df = df[are_under_the_wind]\n",
        "    # suppression\n",
        "    df['random'] = np.random.uniform(0, 1, size=len(df))\n",
        "    are_not_suppressed = df['random'] > suppression_threshold\n",
        "    # print(\"fire suppressed ? {}\".format(list(dict.fromkeys(list(are_not_suppressed)))))\n",
        "    df = df[are_not_suppressed]\n",
        "\n",
        "    # spread fire based on condition\n",
        "    fire_df = df\n",
        "    # fire_df = df[are_neighbors & are_under_the_wind & are_not_suppressed]  # issues with \"are_under_the_wind\n",
        "    # print(len(fire_df.head(5)))\n",
        "    # print(len(fire_df))\n",
        "    list_burn.extend(list(list_fires))\n",
        "    fire_df['step'] = step_value\n",
        "    fire_df.to_csv(os.path.join(path_output, \"step{}_fire.csv\".format(step_value)))\n",
        "    list_fires = list(dict.fromkeys(list(fire_df.target)))\n",
        "    list_burn.extend(list(fire_df.target))\n",
        "    list_burn = list(dict.fromkeys(list_burn))\n",
        "    return list_fires, list_burn\n",
        "\n",
        "\n",
        "def log_files_concatenate(prefix, scenario_count):\n",
        "    list_df = []\n",
        "    files = glob.glob(os.path.join(path_output, prefix))\n",
        "    if files:\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(os.path.join(path_output, file))\n",
        "            list_df.append(df)\n",
        "            os.remove(file)\n",
        "        data = pd.concat(list_df)\n",
        "        data['scenario'] = scenario_count\n",
        "        data.to_csv(os.path.join(path_output, \"fire_scenario_{}.csv\".format(scenario_count)))\n",
        "    else:\n",
        "        print(\"no files to concatenate\")\n",
        "\n",
        "\n",
        "def clean_up_file(prefix, path_path=path_output):\n",
        "    files = glob.glob(os.path.join(path_path, prefix))\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        os.remove(file)\n",
        "\n",
        "\n",
        "def postprocessing(scenarios_recorded, burned_asset, edge_list, gdf_polygons):\n",
        "    list_of_tuples = list(zip(scenarios_recorded, burned_asset))\n",
        "    df = pd.DataFrame(list_of_tuples, columns=['scenarios', 'burned_asset_index'])\n",
        "    # df['count'] = df['burned_asset_index'].value_counts().values\n",
        "    df['count'] = df.groupby('burned_asset_index')['burned_asset_index'].transform('count')\n",
        "    print(df.describe())\n",
        "    df = df[['burned_asset_index', 'count']].drop_duplicates()\n",
        "    edge = edge_list[\n",
        "        ['source', 'source_TARGET_FID', 'source_X', 'source_Y', 'source_geometry']]\n",
        "    df_id = pd.merge(df, edge, left_on='burned_asset_index', right_on='source', how='left')\n",
        "    # print(list(df_id))\n",
        "    df_count = pd.merge(gdf_polygons, df_id, left_on='TARGET_FID', right_on='source_TARGET_FID', how='outer')\n",
        "    df_count = df_count.drop_duplicates()\n",
        "    dataframe = pd.DataFrame(df_count.drop(columns=['geometry', 'source_geometry']))\n",
        "    dataframe = dataframe.dropna()\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    df_count.plot(column='count', cmap='RdYlBu_r', ax=ax, legend=True)\n",
        "    ax.title.set_text(\"Burned buildings after {} scenarios\".format(max(scenarios_recorded)))\n",
        "    plt.savefig(os.path.join(path_output, \"results_{}.png\".format(number_of_scenarios)))\n",
        "    # plt.show()\n",
        "    plt.close(fig)\n",
        "    df_count = df_count.drop(columns=['source', 'source_TARGET_FID', 'source_X', 'source_Y', 'source_geometry'])\n",
        "    df_count.to_csv(os.path.join(path_output, \"results.csv\"))\n",
        "    # df_count.to_file(os.path.join(path_output, \"results.shp\"))\n",
        "    return df_count, dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHk5BlUnjC2",
        "colab_type": "text"
      },
      "source": [
        "Check input to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkGjlJC9oIiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tl0 = datetime.datetime.now()\n",
        "# set up & load input data\n",
        "# gdf = load_data(\"buildings_raw_pts.shp\", 1748570, 5426959, 1748841, 5427115)\n",
        "gdf_polygon = load_data(\"buildings_raw.shp\", 1748412, 5426564, 1749086, 5427606) # smaller\n",
        "# gdf_polygon = load_data(\"buildings_raw.shp\", 1747550, 5426440, 1748813, 5428346) # comparison\n",
        "gdf_polygon[\"area\"] = gdf_polygon['geometry'].area  # m2\n",
        "gdf = gdf_polygon.copy()\n",
        "gdf['geometry'] = gdf['geometry'].centroid\n",
        "gdf['X'] = gdf.centroid.x\n",
        "gdf['Y'] = gdf.centroid.y\n",
        "gdf['d_short'] = gdf_polygon.exterior.distance(gdf)\n",
        "gdf['d_long'] = gdf['area'] / gdf['d_short']\n",
        "\n",
        "# create edge list and network\n",
        "edges = build_edge_list(gdf, 45, gdf_polygon)\n",
        "\n",
        "# create edges\n",
        "G = create_network(edges)\n",
        "\n",
        "tl1 = datetime.datetime.now()\n",
        "print(\"creating edge list took : {}\".format(tl1 - tl0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUOxE4MKoZjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "# set number of scenarios\n",
        "number_of_scenarios = 1000\n",
        "# display of the input data\n",
        "print(\"{} assets loaded\".format(len(gdf)))\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "# gdf.plot(column='area', cmap='hsv', ax=ax[0, 0], legend=True)\n",
        "gdf_polygon.plot(column='area', cmap='hsv', ax=ax[0, 0], legend=True)\n",
        "# gdf.plot(column='TARGET_FID', cmap='hsv', ax=ax[1, 0], legend=True)\n",
        "options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "               'with_labels': False, 'font_weight': 'bold'}\n",
        "nx.draw_kamada_kawai(G, **options, ax=ax[1, 1])\n",
        "ax[0,0].title.set_text(\"area\")\n",
        "# ax[0,1].title.set_text(\"area\")\n",
        "# ax[1,0].title.set_text('FID')\n",
        "ax[1,0].title.set_text('Network display')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(path_output, \"inputs_{}.png\".format(number_of_scenarios)))\n",
        "plt.show()\n",
        "# plt.close(fig)\n",
        "################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsItn72rptyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run model\n",
        "clean_up_file(\"*csv\")\n",
        "clean_up_file(\"*png\")\n",
        "scenarios_list = []\n",
        "log_burned = []  # no removing duplicate\n",
        "# --- SCENARIOS\n",
        "t = datetime.datetime.now()\n",
        "for scenario in range(number_of_scenarios):\n",
        "    t0 = datetime.datetime.now()\n",
        "    burn_list = []\n",
        "    print(\"--- SCENARIO : {}\".format(scenario))\n",
        "    # print(\"initiate fire\")\n",
        "    fire_list = set_initial_fire_to(edges)\n",
        "    x = fire_list\n",
        "    # print(\"fire list : {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "    # print(\"fires list in scenario loop: {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "    if len(fire_list) == 0:\n",
        "        print(\"no fire\")\n",
        "        continue\n",
        "    w_direction, w_speed, w_bearing = wind_scenario()\n",
        "    # print((\"critical distance : {}, wind bearing : {}\".format(w_speed, w_bearing)))\n",
        "    # --------- STEPS\n",
        "    for step in range(len(edges)):\n",
        "        print(\"--------- STEP : {}\".format(step))\n",
        "        fire_list = set_fire_to(edges, fire_list)\n",
        "        y = fire_list\n",
        "        # print(\"fire datasets are identical with initial fire : {}\".format(set(x) == set(y)))\n",
        "        # print(\"fire list : {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "        # print(\"burn list : {}, length : {}\".format(burn_list, len(burn_list)))\n",
        "        # print(\"spread fire\")\n",
        "        fire_list, burn_list = fire_spreading(fire_list, burn_list, w_speed, w_bearing, 0, step, edges)\n",
        "        if len(fire_list) == 0:\n",
        "            # print(\"no fires\")\n",
        "            break\n",
        "        # print(\"fires list : {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "        # print(\"burn list : {}, length : {}\".format(burn_list, len(burn_list)))\n",
        "    log_burned.extend(burn_list)\n",
        "    scenarios_list.extend([scenario] * len(burn_list))\n",
        "    # print(\"log all burn list : {}, length : {}\".format(log_burned, len(log_burned)))\n",
        "    # print(scenarios_list)\n",
        "\n",
        "    log_files_concatenate('step*', scenario)\n",
        "    t1 = datetime.datetime.now()\n",
        "    print(\"..... took : {}\".format(t1 - t0))\n",
        "t2 = datetime.datetime.now()\n",
        "print(\"total time : {}\".format(t2 - t))\n",
        "\n",
        "count_gdf, count_df = postprocessing(scenarios_list, log_bur"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}