{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/37stu37/FFE/blob/master/FFE_network_one_file_run.ipynb",
      "authorship_tag": "ABX9TyMki/631WXnaaeQ542/4POt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_network_one_file_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PipQ4HNTjKW",
        "colab_type": "code",
        "outputId": "a2c72c8f-1d48-4e69-d6a3-98b996f19a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGCGL2eCzyEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a33f4956-b32c-4c85-9b34-77cddf3326fc"
      },
      "source": [
        "%%time \n",
        "%%capture\n",
        "!apt update\n",
        "!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree \n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 226 ms, sys: 57.3 ms, total: 283 ms\n",
            "Wall time: 31.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0PZxCvmWCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b77ca6e9-b9d1-4e62-db58-f1ab893a03f8"
      },
      "source": [
        "%%time\n",
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import networkx as nx\n",
        "from shapely.geometry import Point\n",
        "import imageio\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 83 µs, sys: 0 ns, total: 83 µs\n",
            "Wall time: 89.2 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tMnwAEmqpF",
        "colab_type": "text"
      },
      "source": [
        "Set up the path  to data and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tLiFKrmZDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/05_Sync/FFE/Mesa'\n",
        "path_output = '/content/drive/My Drive/05_Sync/FFE/Mesa/output'\n",
        "\n",
        "# !ls \"/content/drive/My Drive/05_Sync/FFE/Mesa\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kl25fSmjGN",
        "colab_type": "text"
      },
      "source": [
        "Create the functions to be used by the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZosFepFncaX",
        "colab": {}
      },
      "source": [
        "def load_data(file_name, minx, miny, maxx, maxy):\n",
        "    # crop data\n",
        "    bbox = box(minx, miny, maxx, maxy)\n",
        "    # building point dataset\n",
        "    gdf_buildings = gpd.read_file(os.path.join(path, file_name), bbox=bbox)\n",
        "    # gdf_buildings.IgnProb_bl = 0.02\n",
        "    # xmin,ymin,xmax,ymax = gdf_buildings.total_bounds\n",
        "    return gdf_buildings\n",
        "\n",
        "\n",
        "def wind_scenario():\n",
        "    wind_data = pd.read_csv(os.path.join(path, 'GD_wind.csv'))\n",
        "    i = np.random.randint(0, wind_data.shape[0])\n",
        "    w = wind_data.iloc[i, 2]\n",
        "    d = wind_data.iloc[i, 1]\n",
        "    b = wind_data.iloc[i, 3]\n",
        "    return w, d, b\n",
        "\n",
        "\n",
        "def eudistance(v1, v2):\n",
        "    return np.linalg.norm(v1 - v2)\n",
        "\n",
        "\n",
        "def calculate_azimuth(x1, y1, x2, y2):\n",
        "    azimuth = math.degrees(math.atan2((x2 - x1), (y2 - y1)))\n",
        "    return 360 + azimuth\n",
        "\n",
        "\n",
        "def plot(df, column_df):\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    df.plot(column=column_df, ax=ax, legend=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def build_edge_list(geodataframe, maximum_distance, polygon_file):\n",
        "    # create arrays for different id combination\n",
        "    n = np.arange(0, len(geodataframe))\n",
        "    target = [n] * len(geodataframe)\n",
        "    target = np.hstack(target)\n",
        "    source = np.repeat(n, len(geodataframe))\n",
        "    # put arrays in dataframe\n",
        "    df = pd.DataFrame()\n",
        "    df['source_id'] = source\n",
        "    df['target_id'] = target\n",
        "    # merge source attributes with source index\n",
        "    geo_df = geodataframe.copy()\n",
        "    geo_df['id'] = geo_df.index\n",
        "    # create source / target gdf from gdf.columns of interest\n",
        "    geo_df = geo_df[['id', 'TARGET_FID', 'X', 'Y', 'geometry', 'IgnProb_bl']]\n",
        "    geo_df_TRG = geo_df.copy()\n",
        "    geo_df_TRG.columns = ['target_' + str(col) for col in geo_df_TRG.columns]\n",
        "    geo_df_SRC = geo_df.copy()\n",
        "    geo_df_SRC.columns = ['source_' + str(col) for col in geo_df_SRC.columns]\n",
        "    # merge data\n",
        "    merged_data = pd.merge(df, geo_df_SRC, left_on='source_id', right_on='source_id', how='outer')\n",
        "    merged_data = pd.merge(merged_data, geo_df_TRG, left_on='target_id', right_on='target_id', how='outer')\n",
        "    merged_data.rename(columns={'source_id': 'source', 'target_id': 'target'}, inplace=True)\n",
        "    # calculate distance for each source / target pair\n",
        "    # create a df from polygon shape to get accurate distance\n",
        "    # print(list(polygon_file))\n",
        "    polygon = polygon_file[['TARGET_FID', 'geometry']]\n",
        "    # print(list(polygon))\n",
        "    source_poly = merged_data[['source_TARGET_FID']]\n",
        "    target_poly = merged_data[['target_TARGET_FID']]\n",
        "    # print(list(source_poly))\n",
        "    src_poly = pd.merge(source_poly, polygon, left_on='source_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    trg_poly = pd.merge(target_poly, polygon, left_on='target_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    src_poly_gdf = gpd.GeoDataFrame(src_poly, geometry='geometry')\n",
        "    trg_poly_gdf = gpd.GeoDataFrame(trg_poly, geometry='geometry')\n",
        "    distance_series = src_poly_gdf.distance(trg_poly_gdf)\n",
        "    # print(distance_series)\n",
        "\n",
        "    # insert distance in merged data column\n",
        "    merged_data['v1'] = merged_data.source_X - merged_data.target_X\n",
        "    merged_data['v2'] = merged_data.source_Y - merged_data.target_Y\n",
        "    # merged_data['euc_distance'] = np.hypot(merged_data.v1, merged_data.v2)\n",
        "    merged_data['euc_distance'] = distance_series\n",
        "    # remove when distance \"illegal\"\n",
        "    valid_distance = merged_data['euc_distance'] < maximum_distance\n",
        "    not_same_node = merged_data['euc_distance'] != 0\n",
        "    data = merged_data[valid_distance & not_same_node]\n",
        "    # calculate azimuth\n",
        "    data['azimuth'] = np.degrees(np.arctan2(merged_data['v2'], merged_data['v1']))\n",
        "    data['bearing'] = (data.azimuth + 360) % 360\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_network(edge_list_dataframe):\n",
        "    graph = nx.from_pandas_edgelist(edge_list_dataframe, edge_attr=True)\n",
        "    # options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "    #            'with_labels': False, 'font_weight': 'bold'}\n",
        "    # nx.draw_kamada_kawai(graph, **options)\n",
        "    # plt.show()\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjmAYO9UnNOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_initial_fire_to(df):\n",
        "    \"\"\"Fine = 0, Fire = 1, Burned = 2\"\"\"\n",
        "    df['RNG'] = np.random.uniform(0, 1, size=len(df))  # add for random suppression per building, df.shape[0])\n",
        "    onFire = df['source_IgnProb_bl'] > df['RNG']\n",
        "    ignitions = df[onFire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(ignitions.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def set_fire_to(df, existing_fires):\n",
        "    are_set_on_fire = (df['source'].isin(existing_fires))\n",
        "    spark = df[are_set_on_fire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(spark.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def fire_spreading(list_fires, list_burn, wind_speed, wind_bearing, suppression_threshold, step_value, data):\n",
        "    # check the fire potential targets\n",
        "    # print(\"fire list before spreading : {}, length : {}\".format(fire_list, len(fire_list)))\n",
        "    are_potential_targets = (data['source'].isin(list_fires))\n",
        "    are_not_already_burned = (~data['target'].isin(list_burn))\n",
        "    df = data[are_potential_targets & are_not_already_burned]\n",
        "    if df.empty:\n",
        "        # print(\"no fires\")\n",
        "        list_burn.extend(list(list_fires))\n",
        "        list_burn = list(dict.fromkeys(list_burn))\n",
        "        return [], list_burn  # to break the step loop\n",
        "    # set up additional CONDITIONS for fire spreading\n",
        "\n",
        "    # neighbors selection from buffer\n",
        "    df['buffer_geometry'] = gdf.geometry.buffer(gdf['d_long'] + wind_speed)\n",
        "\n",
        "    are_neighbors = df['euc_distance'] < wind_speed\n",
        "    # print(\"neighbors affected ? {}\".format(list(dict.fromkeys(list(are_neighbors)))))\n",
        "    df = df[are_neighbors]\n",
        "    # wind direction\n",
        "    wind_bearing_max = wind_bearing + 45\n",
        "    wind_bearing_min = wind_bearing - 45\n",
        "    if wind_bearing == 360:\n",
        "        wind_bearing_max = 45\n",
        "    if wind_bearing <= 0:  # should not be necessary\n",
        "        wind_bearing_min = 0\n",
        "    if wind_bearing == 999:\n",
        "        wind_bearing_max = 999\n",
        "        wind_bearing_min = 0\n",
        "    are_under_the_wind = (df['bearing'] < wind_bearing_max) & (df['bearing'] > wind_bearing_min)\n",
        "    # print(\"targets under the wind ? {}\".format(list(dict.fromkeys(list(are_under_the_wind)))))\n",
        "    df = df[are_under_the_wind]\n",
        "    # suppression\n",
        "    df['random'] = np.random.uniform(0, 1, size=len(df))\n",
        "    are_not_suppressed = df['random'] > suppression_threshold\n",
        "    # print(\"fire suppressed ? {}\".format(list(dict.fromkeys(list(are_not_suppressed)))))\n",
        "    df = df[are_not_suppressed]\n",
        "\n",
        "    # spread fire based on condition\n",
        "    fire_df = df\n",
        "    # fire_df = df[are_neighbors & are_under_the_wind & are_not_suppressed]  # issues with \"are_under_the_wind\n",
        "    # print(len(fire_df.head(5)))\n",
        "    # print(len(fire_df))\n",
        "    list_burn.extend(list(list_fires))\n",
        "    fire_df['step'] = step_value\n",
        "    fire_df.to_csv(os.path.join(path_output, \"step{}_fire.csv\".format(step_value)))\n",
        "    list_fires = list(dict.fromkeys(list(fire_df.target)))\n",
        "    list_burn.extend(list(fire_df.target))\n",
        "    list_burn = list(dict.fromkeys(list_burn))\n",
        "    return list_fires, list_burn\n",
        "\n",
        "\n",
        "def log_files_concatenate(prefix, scenario_count):\n",
        "    list_df = []\n",
        "    files = glob.glob(os.path.join(path_output, prefix))\n",
        "    if files:\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(os.path.join(path_output, file))\n",
        "            list_df.append(df)\n",
        "            os.remove(file)\n",
        "        data = pd.concat(list_df)\n",
        "        data['scenario'] = scenario_count\n",
        "        data.to_csv(os.path.join(path_output, \"fire_scenario_{}.csv\".format(scenario_count)))\n",
        "    else:\n",
        "        print(\"no files to concatenate\")\n",
        "\n",
        "\n",
        "def clean_up_file(prefix, path_path=path_output):\n",
        "    files = glob.glob(os.path.join(path_path, prefix))\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        os.remove(file)\n",
        "\n",
        "\n",
        "def postprocessing(scenarios_recorded, burned_asset, edge_list, gdf_polygons):\n",
        "    list_of_tuples = list(zip(scenarios_recorded, burned_asset))\n",
        "    df = pd.DataFrame(list_of_tuples, columns=['scenarios', 'burned_asset_index'])\n",
        "    # df['count'] = df['burned_asset_index'].value_counts().values\n",
        "    df['count'] = df.groupby('burned_asset_index')['burned_asset_index'].transform('count')\n",
        "    print(df.describe())\n",
        "    df = df[['burned_asset_index', 'count']].drop_duplicates()\n",
        "    edge = edge_list[\n",
        "        ['source', 'source_TARGET_FID', 'source_X', 'source_Y', 'source_geometry']]\n",
        "    df_id = pd.merge(df, edge, left_on='burned_asset_index', right_on='source', how='left')\n",
        "    # print(list(df_id))\n",
        "    df_count = pd.merge(gdf_polygons, df_id, left_on='TARGET_FID', right_on='source_TARGET_FID', how='outer')\n",
        "    df_count = df_count.drop_duplicates()\n",
        "    dataframe = pd.DataFrame(df_count.drop(columns=['geometry', 'source_geometry']))\n",
        "    dataframe = dataframe.dropna()\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    df_count.plot(column='count', cmap='RdYlBu_r', ax=ax, legend=True)\n",
        "    ax.title.set_text(\"Burned buildings after {} scenarios\".format(max(scenarios_recorded)))\n",
        "    plt.savefig(os.path.join(path_output, \"results_{}.png\".format(number_of_scenarios)))\n",
        "    # plt.show()\n",
        "    plt.close(fig)\n",
        "    df_count = df_count.drop(columns=['source', 'source_TARGET_FID', 'source_X', 'source_Y', 'source_geometry'])\n",
        "    df_count.to_csv(os.path.join(path_output, \"results.csv\"))\n",
        "    # df_count.to_file(os.path.join(path_output, \"results.shp\"))\n",
        "    return df_count, dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHk5BlUnjC2",
        "colab_type": "text"
      },
      "source": [
        "Check input to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfNpBnSi7J-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_edge_list_itertuple(geodataframe, maximum_distance):\n",
        "  list_dataframe = []\n",
        "  for row in geodataframe.itertuples():\n",
        "    print(row.TARGET_FID)\n",
        "    source_list = row.TARGET_FID.repeat(len(geodataframe))\n",
        "    target_list = geodataframe[TARGET_FID]\n",
        "    frame = { 'Source': source_list, 'Target': target_list } \n",
        "    df = pd.DataFrame(frame)\n",
        "    # calculate distance based on polygon\n",
        "    polygons = geodataframe[['TARGET_FID', 'geometry']]\n",
        "    source_poly = polygons[polygons['TARGET_FID'].isin(source_list)]\n",
        "    source_poly = pd.merge(source_list, polygon, left_on='TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    source_poly = gpd.GeoDataFrame(source_poly, geometry='geometry')\n",
        "    target_poly = polygons[polygons['TARGET_FID'].isin(target_list)]\n",
        "    target_poly = pd.merge(target_list, polygon, left_on='TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    target_poly = gpd.GeoDataFrame(target_poly, geometry='geometry')\n",
        "    \n",
        "    distance_series = source_poly.distance(target_poly)\n",
        "    df['euc_distance'] = distance_series\n",
        "    # calculate bearing based on polygon centroid\n",
        "\n",
        "    return\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkGjlJC9oIiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c7393746-aeb0-47b2-da5d-5b548b646490"
      },
      "source": [
        "%%time\n",
        "# set up & load input data\n",
        "# gdf = load_data(\"buildings_raw_pts.shp\", 1748570, 5426959, 1748841, 5427115)\n",
        "gdf_polygon = load_data(\"buildings_raw.shp\", 1748412, 5426564, 1749086, 5427606) # smaller\n",
        "# gdf_polygon = load_data(\"buildings_raw.shp\", 1747550, 5426440, 1748813, 5428346) # comparison\n",
        "gdf_polygon[\"area\"] = gdf_polygon['geometry'].area  # m2\n",
        "gdf = gdf_polygon.copy()\n",
        "gdf['geometry'] = gdf['geometry'].centroid\n",
        "gdf['X'] = gdf.centroid.x\n",
        "gdf['Y'] = gdf.centroid.y\n",
        "gdf['d_short'] = gdf_polygon.exterior.distance(gdf)\n",
        "gdf['d_long'] = gdf['area'] / gdf['d_short']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 720 ms, sys: 42 ms, total: 762 ms\n",
            "Wall time: 3.31 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z4jZW6Z56Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tl0 = datetime.datetime.now()\n",
        "# create edge list and network\n",
        "edges = build_edge_list(gdf, 45, gdf_polygon)\n",
        "\n",
        "# create edges\n",
        "G = create_network(edges)\n",
        "\n",
        "tl1 = datetime.datetime.now()\n",
        "print(\"creating edge list took : {}\".format(tl1 - tl0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}