{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1v1lbaBCbEeaRXlLWebf43rMrzybni93G",
      "authorship_tag": "ABX9TyP4KcEimf9cemTslRFWBx0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_postprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38DjDo5m4fUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f65c2744-e7a2-4ff6-9d86-aa6d785e3704"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHUpIqTnsn8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6cf0c67-596c-41b1-c12e-da1c75cb0f04"
      },
      "source": [
        "%%time \n",
        "%%capture\n",
        "!apt update\n",
        "!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree \n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes \n",
        "!pip install memory_profiler"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.48 s, sys: 282 ms, total: 1.77 s\n",
            "Wall time: 31 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIHmXBmk4kLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6b79287-f042-4df2-ed30-30868a39e13b"
      },
      "source": [
        "%%time\n",
        "import datetime\n",
        "import glob\n",
        "import shutil\n",
        "from math import sqrt\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import bokeh\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from scipy.spatial import distance\n",
        "from shapely.geometry import box\n",
        "from shapely.geometry import shape\n",
        "from shapely.geometry import Point\n",
        "import networkx as nx\n",
        "from sys import getsizeof\n",
        "from numba import jit\n",
        "import dask.dataframe as dd\n",
        "import dask.array as da\n",
        "import dask\n",
        "from dask.distributed import Client\n",
        "from dask.diagnostics import ProgressBar\n",
        "%matplotlib inline\n",
        "%load_ext memory_profiler\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The memory_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext memory_profiler\n",
            "CPU times: user 4.55 ms, sys: 0 ns, total: 4.55 ms\n",
            "Wall time: 4.86 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHzvowFt4lAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "c65ca4f7-58cf-4d73-cea8-ae2c62256ec4"
      },
      "source": [
        "client = Client(processes=False)\n",
        "client"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/distributed/bokeh/core.py:57: UserWarning: \n",
            "Failed to start diagnostics server on port 8787. [Errno 99] Cannot assign requested address\n",
            "  warnings.warn('\\n' + msg)\n",
            "/usr/local/lib/python3.6/dist-packages/distributed/deploy/local.py:197: UserWarning: \n",
            "Could not launch service 'bokeh' on port 8787. Got the following message:\n",
            "\n",
            "[Errno 99] Cannot assign requested address\n",
            "  self.scheduler.start(scheduler_address)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table style=\"border: 2px solid white;\">\n",
              "<tr>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Client</h3>\n",
              "<ul>\n",
              "  <li><b>Scheduler: </b>inproc://172.28.0.2/122/8\n",
              "</ul>\n",
              "</td>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Cluster</h3>\n",
              "<ul>\n",
              "  <li><b>Workers: </b>1</li>\n",
              "  <li><b>Cores: </b>2</li>\n",
              "  <li><b>Memory: </b>13.66 GB</li>\n",
              "</ul>\n",
              "</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<Client: scheduler='inproc://172.28.0.2/122/8' processes=1 cores=2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYH84Tq4r4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input'\n",
        "path_output = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKwRdpcN4t1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @dask.delayed\n",
        "# def read_and_concatenate_parquets(prefix, number_of_scenario, path=path_output):\n",
        "  L = []\n",
        "  files = glob.glob(os.path.join(path_output, prefix))# output_scenario_0_step_0.parquet\n",
        "  for file in files:\n",
        "    pqt = dd.read_parquet(file)\n",
        "    L.append(pqt)\n",
        "  df = dd.concat(L)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDoaZtl4IP2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def count_fid_occurences(df):\n",
        "  count = df.source.value_counts().compute()\n",
        "  count_df = count.to_frame()\n",
        "  count_df['count'] = count\n",
        "  count_df['source'] = count_df.index\n",
        "  count_df = count_df.reset_index()\n",
        "  count_df = count_df.drop(columns='index')\n",
        "  return count_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Kc8edMMXNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_shapefile(file_name, minx, miny, maxx, maxy):\n",
        "    # crop data\n",
        "    bbox = box(minx, miny, maxx, maxy)\n",
        "    # building point dataset\n",
        "    gdf_buildings = gpd.read_file(os.path.join(path, file_name), bbox=bbox)\n",
        "    max_extent = gdf_buildings.total_bounds\n",
        "    data_size = getsizeof(gdf_buildings) /(1024.0**3)\n",
        "    print(\"Shapefile extent : {}\".format(max_extent))\n",
        "    print(\"Asset loaded : {}\".format(len(gdf_buildings)))\n",
        "    # gdf.plot(column='IgnProb_bl', cmap='hsv', legend=True)\n",
        "    return gdf_buildings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aynSO44CJcQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_coordinates_export_shape(ddf, gdf, name_output):\n",
        "  gdf = gdf[['TARGET_FID', 'geometry']]\n",
        "  df = pd.DataFrame(gdf)\n",
        "  ddf = ddf.compute()\n",
        "  df_merge = ddf.merge(df, how='left', left_on='source', right_on='TARGET_FID')\n",
        "  gdf_merge = gpd.GeoDataFrame(df_merge, geometry='geometry')\n",
        "  gdf_merge.plot(column='count', cmap='seismic', legend=True)\n",
        "  gdf_merge.to_file(os.path.join(path_output, \"shapefiles\", name_output + \".shp\"))\n",
        "  return gdf_merge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pb6yiAn-OPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# %memit\n",
        "# df = read_and_concatenate_parquets(\"output_scenario*\")\n",
        "# count_df = count_fid_occurences(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sg0KktJMk4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdf = load_shapefile(\"buildings_raw.shp\", 1740508, 5420049, 1755776, 5443033) # whole"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBQTH3k6O_Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdf_count = merge_coordinates_export_shape(count_df, gdf, \"burned_buildings\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOCS3ae3YP6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.plotting import figure\n",
        "from bokeh.io import show, output_notebook\n",
        "\n",
        "# Create the blank plot\n",
        "p = figure(plot_height = 600, plot_width = 600, \n",
        "           title = 'Histogram of Burn Count',\n",
        "          x_axis_label = 'Delay (min)]', \n",
        "           y_axis_label = 'Number of Burned Buildings')\n",
        "\n",
        "# Add a quad glyph\n",
        "hist, edges = np.histogram(count_df, density=True, bins=50)\n",
        "p.quad(top=hist, bottom=0, fill_color='red', line_color='black') # left=edges[:-1], right=edges[1:], \n",
        "\n",
        "# Set to output the plot in the notebook\n",
        "output_notebook()\n",
        "# Show the plot\n",
        "show(p)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}