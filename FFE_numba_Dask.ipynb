{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/37stu37/FFE/blob/master/FFE_network_one_file_run.ipynb",
      "authorship_tag": "ABX9TyNCevVTAS4Hj6nfxJvp9r9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_numba_Dask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGCGL2eCzyEv",
        "colab_type": "code",
        "outputId": "215b9486-1d18-4a9d-fc5c-94f3a707e390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time \n",
        "%%capture\n",
        "!apt update\n",
        "!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree \n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 148 ms, sys: 40 ms, total: 188 ms\n",
            "Wall time: 22 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PipQ4HNTjKW",
        "colab_type": "code",
        "outputId": "20f2dc58-6424-48d0-97be-aa9113152168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0PZxCvmWCl",
        "colab_type": "code",
        "outputId": "e49e1fca-613d-4417-e175-a30419434867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "import datetime\n",
        "import glob\n",
        "from math import sqrt\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from scipy.spatial import distance\n",
        "from shapely.geometry import box\n",
        "from shapely.geometry import shape\n",
        "from shapely.geometry import Point\n",
        "import networkx as nx\n",
        "from sys import getsizeof\n",
        "from numba import jit\n",
        "import dask.dataframe as dd\n",
        "import dask.array as da\n",
        "import dask\n",
        "from dask.distributed import Client\n",
        "from dask.diagnostics import ProgressBar\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 678 ms, sys: 69.4 ms, total: 748 ms\n",
            "Wall time: 773 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz4lyV9f3Sny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "d7dea8d7-4cd0-4a8d-8592-1c3aac2e7023"
      },
      "source": [
        "client = Client(processes=False)\n",
        "client"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/distributed/bokeh/core.py:57: UserWarning: \n",
            "Failed to start diagnostics server on port 8787. [Errno 99] Cannot assign requested address\n",
            "  warnings.warn('\\n' + msg)\n",
            "/usr/local/lib/python3.6/dist-packages/distributed/deploy/local.py:197: UserWarning: \n",
            "Could not launch service 'bokeh' on port 8787. Got the following message:\n",
            "\n",
            "[Errno 99] Cannot assign requested address\n",
            "  self.scheduler.start(scheduler_address)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table style=\"border: 2px solid white;\">\n",
              "<tr>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Client</h3>\n",
              "<ul>\n",
              "  <li><b>Scheduler: </b>inproc://172.28.0.2/3068/1\n",
              "</ul>\n",
              "</td>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Cluster</h3>\n",
              "<ul>\n",
              "  <li><b>Workers: </b>1</li>\n",
              "  <li><b>Cores: </b>2</li>\n",
              "  <li><b>Memory: </b>13.66 GB</li>\n",
              "</ul>\n",
              "</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<Client: scheduler='inproc://172.28.0.2/3068/1' processes=1 cores=2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tMnwAEmqpF",
        "colab_type": "text"
      },
      "source": [
        "Set up the path  to data and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tLiFKrmZDM",
        "colab_type": "code",
        "outputId": "7d6ed9bd-743b-484d-ffa4-9a6db4700660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "path = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input'\n",
        "path_output = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_output'\n",
        "\n",
        "!ls \"/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buildings_raw.cpg      buildings_raw_pts.mshp  buildings_raw.shp\n",
            "buildings_raw.dbf      buildings_raw_pts.prj   buildings_raw.shx\n",
            "buildings_raw.prj      buildings_raw_pts.shp   GD_wind.csv\n",
            "buildings_raw_pts.cpg  buildings_raw_pts.shx   source_target.csv\n",
            "buildings_raw_pts.dbf  buildings_raw.qpj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kl25fSmjGN",
        "colab_type": "text"
      },
      "source": [
        "Create the functions to be used by the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYX1StPx6XfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(file_name, shapefile_name):\n",
        "    gdf = gpd.read_file(os.path.join(path, shapefile_name))\n",
        "    gdf.plot()\n",
        "    gdf_short = gdf[['TARGET_FID', 'geometry', 'IgnProb_bl']]\n",
        "    df_short = pd.DataFrame(gdf_short)\n",
        "    ddf = dd.read_parquet(os.path.join(path_output, 'dask_edge_list', file_name), engine='pyarrow')\n",
        "    ddf = ddf.compute()\n",
        "\n",
        "    # merge column of interest (geometries and Ignition probability)\n",
        "    df_source = ddf.merge(df_short, how='left', left_on='source', right_on='TARGET_FID')\n",
        "    # gdf_short = gdf[['TARGET_FID', 'geometry', 'IgnProb_bl']]\n",
        "    # df_short = pd.DataFrame(gdf_short)\n",
        "    df_target = ddf.merge(df_short, how='left', left_on='target', right_on='TARGET_FID')\n",
        "\n",
        "    # reverse to geopandas\n",
        "    # df_source = df_source.compute()\n",
        "    # df_target = df_target.compute()\n",
        "\n",
        "    gdf_source = gpd.GeoDataFrame(df_source, geometry='geometry')\n",
        "    gdf_target = gpd.GeoDataFrame(df_target, geometry='geometry')\n",
        "\n",
        "    # calculate distance polygons\n",
        "    distance_series = gdf_source.distance(gdf_target)\n",
        "    ddf['distance'] = distance_series\n",
        "\n",
        "    return ddf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZosFepFncaX",
        "colab": {}
      },
      "source": [
        "def plot_network(edge_list_dataframe):\n",
        "    graph = nx.from_pandas_edgelist(edge_list_dataframe, edge_attr=True)\n",
        "    options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "               'with_labels': False, 'font_weight': 'bold'}\n",
        "    nx.draw_kamada_kawai(graph, **options)\n",
        "    plt.show()\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9H08CSY1fhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wind_scenario():\n",
        "    wind_data = pd.read_csv(os.path.join(path, 'GD_wind.csv'))\n",
        "    i = np.random.randint(0, wind_data.shape[0])\n",
        "    w = wind_data.iloc[i, 2]\n",
        "    d = wind_data.iloc[i, 1]\n",
        "    b = wind_data.iloc[i, 3]\n",
        "    return w, d, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjmAYO9UnNOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_initial_fire_to(df):\n",
        "    \"\"\"Fine = 0, Fire = 1, Burned = 2\"\"\"\n",
        "    df['RNG'] = np.random.uniform(0, 1, size=len(df))  # add for random suppression per building, df.shape[0])\n",
        "    onFire = df['IgnProb_bl'] > df['RNG']\n",
        "    ignitions = df[onFire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(ignitions.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def set_fire_to(df, existing_fires):\n",
        "    are_set_on_fire = (df['source'].isin(existing_fires))\n",
        "    spark = df[are_set_on_fire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(spark.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def fire_spreading(list_fires, list_burn, wind_speed, wind_bearing, suppression_threshold, step_value, data):\n",
        "    # check the fire potential targets\n",
        "    # print(\"fire list before spreading : {}, length : {}\".format(list_fires, len(list_fires)))\n",
        "    are_potential_targets = (data['source'].isin(list_fires))\n",
        "    are_not_already_burned = (~data['target'].isin(list_burn))\n",
        "    df = data[are_potential_targets & are_not_already_burned]\n",
        "    if df.empty:\n",
        "        # print(\"no fires\")\n",
        "        list_burn.extend(list(list_fires))\n",
        "        list_burn = list(dict.fromkeys(list_burn))\n",
        "        return [], list_burn  # to break the step loop\n",
        "    # set up additional CONDITIONS for fire spreading\n",
        "\n",
        "    # neighbors selection from buffer\n",
        "    are_neighbors = df['distance'] < wind_speed\n",
        "    df = df[are_neighbors]\n",
        "\n",
        "    # wind direction\n",
        "    wind_bearing_max = wind_bearing + 45\n",
        "    wind_bearing_min = wind_bearing - 45\n",
        "    if wind_bearing == 360:\n",
        "        wind_bearing_max = 45\n",
        "    if wind_bearing <= 0:  # should not be necessary\n",
        "        wind_bearing_min = 0\n",
        "    if wind_bearing == 999:\n",
        "        wind_bearing_max = 999\n",
        "        wind_bearing_min = 0\n",
        "    are_under_the_wind = (df['bearing'] < wind_bearing_max) & (df['bearing'] > wind_bearing_min)\n",
        "    # print(\"targets under the wind ? {}\".format(list(dict.fromkeys(list(are_under_the_wind)))))\n",
        "    df = df[are_under_the_wind]\n",
        "    # suppression\n",
        "    df['random'] = np.random.uniform(0, 1, size=len(df))\n",
        "    are_not_suppressed = df['random'] > suppression_threshold\n",
        "    # print(\"fire suppressed ? {}\".format(list(dict.fromkeys(list(are_not_suppressed)))))\n",
        "    df = df[are_not_suppressed]\n",
        "\n",
        "    # spread fire based on condition\n",
        "    fire_df = df\n",
        "    # fire_df = df[are_neighbors & are_under_the_wind & are_not_suppressed]  # issues with \"are_under_the_wind\n",
        "    # print(len(fire_df.head(5)))\n",
        "    # print(len(fire_df))\n",
        "    list_burn.extend(list(list_fires))\n",
        "    fire_df['step'] = step_value\n",
        "    # fire_df.to_csv(os.path.join(path_output, \"step{}_fire.csv\".format(step_value))) # ADD IF CSV OUTPUT NEEDED\n",
        "    list_fires = list(dict.fromkeys(list(fire_df.target)))\n",
        "    list_burn.extend(list(fire_df.target))\n",
        "    list_burn = list(dict.fromkeys(list_burn))\n",
        "    return list_fires, list_burn\n",
        "\n",
        "\n",
        "def log_files_concatenate(prefix, scenario_count):\n",
        "    list_df = []\n",
        "    files = glob.glob(os.path.join(path_output, prefix))\n",
        "    if files:\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(os.path.join(path_output, file))\n",
        "            list_df.append(df)\n",
        "            os.remove(file)\n",
        "        data = pd.concat(list_df)\n",
        "        data['scenario'] = scenario_count\n",
        "        data.to_csv(os.path.join(path_output, \"fire_scenario_{}.csv\".format(scenario_count)))\n",
        "    else:\n",
        "        print(\"no files to concatenate\")\n",
        "\n",
        "\n",
        "def clean_up_file(prefix, path_path=path_output):\n",
        "    files = glob.glob(os.path.join(path_path, prefix))\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        os.remove(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8B7CEVY7tMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(number_of_scenarios, edges):\n",
        "  clean_up_file(\"*csv\")\n",
        "  clean_up_file(\"*png\")\n",
        "  scenarios_list = []\n",
        "  log_burned = []  # no removing duplicate\n",
        "  # --- SCENARIOS\n",
        "  t = datetime.datetime.now()\n",
        "  print(\"number of scenarios : {}\".format(number_of_scenarios))\n",
        "  for scenario in range(number_of_scenarios):\n",
        "      t0 = datetime.datetime.now()\n",
        "      burn_list = []\n",
        "      print(\"--- SCENARIO : {}\".format(scenario))\n",
        "      fire_list = set_initial_fire_to(edges)\n",
        "      if len(fire_list) == 0:\n",
        "          print(\"no fire\")\n",
        "          log_burned.extend(burn_list)\n",
        "          scenarios_list.extend([scenario] * len(burn_list))\n",
        "          continue\n",
        "      w_direction, w_speed, w_bearing = wind_scenario()\n",
        "      # --------- STEPS\n",
        "      for step in range(len(edges)):\n",
        "          print(\"--------- STEP : {}\".format(step))\n",
        "          fire_list = set_fire_to(edges, fire_list)\n",
        "          fire_list, burn_list = fire_spreading(fire_list, burn_list, w_speed, w_bearing, 0, step, edges)\n",
        "          if len(fire_list) == 0:\n",
        "              break\n",
        "      log_burned.extend(burn_list)\n",
        "      scenarios_list.extend([scenario] * len(burn_list))\n",
        "\n",
        "      # log_files_concatenate('step*', scenario)\n",
        "      \n",
        "  return scenarios_list, log_burned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7ZFBFEdij7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def postprocessing(scenarios_recorded, burned_asset, gdf_polygons):\n",
        "    list_of_tuples = list(zip(scenarios_recorded, burned_asset))\n",
        "    df = pd.DataFrame(list_of_tuples, columns=['scenarios', 'burned_asset_index'])\n",
        "    # df burn count per asset\n",
        "    df['count'] = df.groupby('burned_asset_index')['burned_asset_index'].transform('count')\n",
        "    print(df.describe())\n",
        "    df = df[['burned_asset_index', 'count']]\n",
        "    df_count = pd.merge(df, gdf_polygon, left_on='burned_asset_index', right_on='TARGET_FID', how='left')\n",
        "    # to geodataframe\n",
        "    crs = gdf_polygon.crs\n",
        "    gdf_count = gpd.GeoDataFrame(df_count, crs=crs, geometry='geometry')\n",
        "    # plot\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    gdf_count.plot(column='count', cmap='RdYlBu_r', ax=ax, legend=True)\n",
        "    ax.title.set_text(\"Burned buildings after {} scenarios\".format(max(scenarios_recorded)+1))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(path_output, \"results_{}.png\".format(max(scenarios_recorded)+1)))\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    gdf_count = gdf_count.drop(columns=['SHAPE_Leng', 'SHAPE_Area', 'AU2013Num', 'IgnProb_bl', 'RandProb'])\n",
        "    df_count.to_csv(os.path.join(path_output, \"results_{}_scenarios.csv\".format(max(scenarios_recorded)+1)))\n",
        "    gdf_count.to_file(os.path.join(path_output, \"results_{}_scenarios.shp\".format(max(scenarios_recorded)+1)))\n",
        "\n",
        "    \n",
        "    return df_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnrsfe6LIm4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# load data\n",
        "EDGES = prepare_data(\"edge_data.parquet\",\"buildings_raw.shp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60wYNOT782fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# run model\n",
        "scos, burns = main(100, EDGES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cwH6dj4Tu1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# generate output\n",
        "count_gdf = postprocessing(scenarios_recorded=scos, burned_asset=burns, gdf_polygons=gdf_polygon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMY_nW6oXo-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# check convergence\n",
        "from itertools import groupby\n",
        "number_of_burns_per_scenarios = [len(list(group)) for key, group in groupby(scos)]\n",
        "cumulative_number_of_burns_per_scenarios = list(np.cumsum(number_of_burns_per_scenarios))\n",
        "scenario_list = list(set(scos))\n",
        "scenario_list = [x+1 for x in scenario_list]\n",
        "\n",
        "average_burn_per_scenario = [c / s for c,s in zip(cumulative_number_of_burns_per_scenarios, scenario_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW0ImekxY5FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(scenario_list, average_burn_per_scenario)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}