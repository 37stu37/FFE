{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMBNZJOTlfjFvTf0llE7W0c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_network_w_Dack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLEIMFiV55eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01d12544-a62f-407f-ff2e-1e35042171e8"
      },
      "source": [
        "%%time \n",
        "%%capture\n",
        "!apt update\n",
        "!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree \n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.12 s, sys: 130 ms, total: 1.25 s\n",
            "Wall time: 3min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0xvou966hTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "683fd353-09f6-42a4-c6cf-f66d82428724"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0PZxCvmWCl",
        "colab_type": "code",
        "outputId": "ced990b0-e0ef-44c9-d1ee-df9b61ce86bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import networkx as nx\n",
        "from shapely.geometry import Point\n",
        "from sys import getsizeof\n",
        "from numba import jit\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 881 ms, sys: 131 ms, total: 1.01 s\n",
            "Wall time: 2.34 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz4lyV9f3Sny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "39fc0400-9ee8-4637-e6ca-9f1ab9207ef5"
      },
      "source": [
        "client = Client(processes=False)\n",
        "client"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/distributed/bokeh/core.py:57: UserWarning: \n",
            "Failed to start diagnostics server on port 8787. [Errno 99] Cannot assign requested address\n",
            "  warnings.warn('\\n' + msg)\n",
            "/usr/local/lib/python3.6/dist-packages/distributed/deploy/local.py:197: UserWarning: \n",
            "Could not launch service 'bokeh' on port 8787. Got the following message:\n",
            "\n",
            "[Errno 99] Cannot assign requested address\n",
            "  self.scheduler.start(scheduler_address)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table style=\"border: 2px solid white;\">\n",
              "<tr>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Client</h3>\n",
              "<ul>\n",
              "  <li><b>Scheduler: </b>inproc://172.28.0.2/139/1\n",
              "</ul>\n",
              "</td>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3>Cluster</h3>\n",
              "<ul>\n",
              "  <li><b>Workers: </b>1</li>\n",
              "  <li><b>Cores: </b>4</li>\n",
              "  <li><b>Memory: </b>27.40 GB</li>\n",
              "</ul>\n",
              "</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<Client: scheduler='inproc://172.28.0.2/139/1' processes=1 cores=4>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tLiFKrmZDM",
        "colab_type": "code",
        "outputId": "91e62a76-93c4-466f-e532-95c23c9ba247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input'\n",
        "path_output = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_output'\n",
        "\n",
        "!ls \"/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buildings_raw.cpg  buildings_raw.prj  buildings_raw.shp  GD_wind.csv\n",
            "buildings_raw.dbf  buildings_raw.qpj  buildings_raw.shx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZosFepFncaX",
        "colab": {}
      },
      "source": [
        "def load_data(file_name, minx, miny, maxx, maxy):\n",
        "    # crop data\n",
        "    bbox = box(minx, miny, maxx, maxy)\n",
        "    # building point dataset\n",
        "    gdf_buildings = gpd.read_file(os.path.join(path, file_name), bbox=bbox)\n",
        "    print(gdf_buildings.dtypes)\n",
        "    max_extent = gdf_buildings.total_bounds\n",
        "    data_size = getsizeof(gdf_buildings)/(1024.0**3)\n",
        "    print(\"Shapefile extent : {}\".format(max_extent))\n",
        "    print(\"Asset loaded : {}\".format(len(gdf_buildings)))\n",
        "    print(\"Data size:{} GB'\".format(data_size))\n",
        "    # gdf_buildings.IgnProb_bl = 0.02\n",
        "    # xmin,ymin,xmax,ymax = gdf_buildings.total_bounds\n",
        "    # Precision of float32 is sufficient for lat and lon\n",
        "    float_columns = ['SHAPE_Leng','SHAPE_Area',\n",
        "                    'IgnProb_bl','RandProb']\n",
        "    gdf_buildings[float_columns] = gdf_buildings[float_columns].astype('float32')\n",
        "    int_columns = ['TARGET_FID','Combustibl',\n",
        "                    'AU2013Num','RandProb']\n",
        "    gdf_buildings[int_columns] = gdf_buildings[int_columns].astype('int32')\n",
        "    data_size = getsizeof(gdf_buildings)/(1024.0**3)\n",
        "    print(\"resized Data size:{} GB'\".format(data_size))\n",
        "    return gdf_buildings\n",
        "\n",
        "\n",
        "def wind_scenario():\n",
        "    wind_data = pd.read_csv(os.path.join(path, 'GD_wind.csv'))\n",
        "    i = np.random.randint(0, wind_data.shape[0])\n",
        "    w = wind_data.iloc[i, 2]\n",
        "    d = wind_data.iloc[i, 1]\n",
        "    b = wind_data.iloc[i, 3]\n",
        "    return w, d, b\n",
        "\n",
        "\n",
        "def create_network(edge_list_dataframe):\n",
        "    graph = nx.from_pandas_edgelist(edge_list_dataframe, edge_attr=True)\n",
        "    # options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "    #            'with_labels': False, 'font_weight': 'bold'}\n",
        "    # nx.draw_kamada_kawai(graph, **options)\n",
        "    # plt.show()\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjmAYO9UnNOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_initial_fire_to(df):\n",
        "    \"\"\"Fine = 0, Fire = 1, Burned = 2\"\"\"\n",
        "    df['RNG'] = np.random.uniform(0, 1, size=len(df))  # add for random suppression per building, df.shape[0])\n",
        "    onFire = df['source_IgnProb_bl'] > df['RNG']\n",
        "    ignitions = df[onFire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(ignitions.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def set_fire_to(df, existing_fires):\n",
        "    are_set_on_fire = (df['source'].isin(existing_fires))\n",
        "    spark = df[are_set_on_fire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(spark.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def fire_spreading(list_fires, list_burn, wind_speed, wind_bearing, suppression_threshold, step_value, data):\n",
        "    # check the fire potential targets\n",
        "    # print(\"fire list before spreading : {}, length : {}\".format(list_fires, len(list_fires)))\n",
        "    are_potential_targets = (data['source'].isin(list_fires))\n",
        "    are_not_already_burned = (~data['target'].isin(list_burn))\n",
        "    df = data[are_potential_targets & are_not_already_burned]\n",
        "    if df.empty:\n",
        "        # print(\"no fires\")\n",
        "        list_burn.extend(list(list_fires))\n",
        "        list_burn = list(dict.fromkeys(list_burn))\n",
        "        return [], list_burn  # to break the step loop\n",
        "    # set up additional CONDITIONS for fire spreading\n",
        "\n",
        "    # neighbors selection from buffer\n",
        "    are_neighbors = df['distance'] < wind_speed\n",
        "    df = df[are_neighbors]\n",
        "\n",
        "    # wind direction\n",
        "    wind_bearing_max = wind_bearing + 45\n",
        "    wind_bearing_min = wind_bearing - 45\n",
        "    if wind_bearing == 360:\n",
        "        wind_bearing_max = 45\n",
        "    if wind_bearing <= 0:  # should not be necessary\n",
        "        wind_bearing_min = 0\n",
        "    if wind_bearing == 999:\n",
        "        wind_bearing_max = 999\n",
        "        wind_bearing_min = 0\n",
        "    are_under_the_wind = (df['bearing'] < wind_bearing_max) & (df['bearing'] > wind_bearing_min)\n",
        "    # print(\"targets under the wind ? {}\".format(list(dict.fromkeys(list(are_under_the_wind)))))\n",
        "    df = df[are_under_the_wind]\n",
        "    # suppression\n",
        "    df['random'] = np.random.uniform(0, 1, size=len(df))\n",
        "    are_not_suppressed = df['random'] > suppression_threshold\n",
        "    # print(\"fire suppressed ? {}\".format(list(dict.fromkeys(list(are_not_suppressed)))))\n",
        "    df = df[are_not_suppressed]\n",
        "\n",
        "    # spread fire based on condition\n",
        "    fire_df = df\n",
        "    # fire_df = df[are_neighbors & are_under_the_wind & are_not_suppressed]  # issues with \"are_under_the_wind\n",
        "    # print(len(fire_df.head(5)))\n",
        "    # print(len(fire_df))\n",
        "    list_burn.extend(list(list_fires))\n",
        "    fire_df['step'] = step_value\n",
        "    # fire_df.to_csv(os.path.join(path_output, \"step{}_fire.csv\".format(step_value))) # ADD IF CSV OUTPUT NEEDED\n",
        "    list_fires = list(dict.fromkeys(list(fire_df.target)))\n",
        "    list_burn.extend(list(fire_df.target))\n",
        "    list_burn = list(dict.fromkeys(list_burn))\n",
        "    return list_fires, list_burn\n",
        "\n",
        "\n",
        "def log_files_concatenate(prefix, scenario_count):\n",
        "    list_df = []\n",
        "    files = glob.glob(os.path.join(path_output, prefix))\n",
        "    if files:\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(os.path.join(path_output, file))\n",
        "            list_df.append(df)\n",
        "            os.remove(file)\n",
        "        data = pd.concat(list_df)\n",
        "        data['scenario'] = scenario_count\n",
        "        data.to_csv(os.path.join(path_output, \"fire_scenario_{}.csv\".format(scenario_count)))\n",
        "    else:\n",
        "        print(\"no files to concatenate\")\n",
        "\n",
        "\n",
        "def clean_up_file(prefix, path_path=path_output):\n",
        "    files = glob.glob(os.path.join(path_path, prefix))\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        os.remove(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV7sg0YI8V4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_edge_list_dack(geodataframe, maximum_distance, polygon_file):\n",
        "    # create arrays for different id combination\n",
        "    n = np.arange(0, len(geodataframe))\n",
        "    target = [n] * len(geodataframe)\n",
        "    target = np.hstack(target)\n",
        "    source = np.repeat(n, len(geodataframe))\n",
        "    # put arrays in dataframe\n",
        "    df = pd.DataFrame()\n",
        "    df['source_id'] = source\n",
        "    df['target_id'] = target\n",
        "    # merge source attributes with source index\n",
        "    geo_df = geodataframe.copy()\n",
        "    geo_df['id'] = geo_df.index\n",
        "    # create source / target gdf from gdf.columns of interest\n",
        "    geo_df = geo_df[['id', 'TARGET_FID', 'X', 'Y', 'geometry', 'IgnProb_bl']]\n",
        "    geo_df_TRG = geo_df.copy()\n",
        "    geo_df_TRG.columns = ['target_' + str(col) for col in geo_df_TRG.columns]\n",
        "    geo_df_SRC = geo_df.copy()\n",
        "    geo_df_SRC.columns = ['source_' + str(col) for col in geo_df_SRC.columns]\n",
        "    # merge data\n",
        "    merged_data = pd.merge(df, geo_df_SRC, left_on='source_id', right_on='source_id', how='outer')\n",
        "    merged_data = pd.merge(merged_data, geo_df_TRG, left_on='target_id', right_on='target_id', how='outer')\n",
        "    merged_data.rename(columns={'source_id': 'source', 'target_id': 'target'}, inplace=True)\n",
        "    # calculate distance for each source / target pair\n",
        "    # create a df from polygon shape to get accurate distance\n",
        "    # print(list(polygon_file))\n",
        "    polygon = polygon_file[['TARGET_FID', 'geometry']]\n",
        "    # print(list(polygon))\n",
        "    source_poly = merged_data[['source_TARGET_FID']]\n",
        "    target_poly = merged_data[['target_TARGET_FID']]\n",
        "    # print(list(source_poly))\n",
        "    src_poly = pd.merge(source_poly, polygon, left_on='source_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    trg_poly = pd.merge(target_poly, polygon, left_on='target_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    src_poly_gdf = gpd.GeoDataFrame(src_poly, geometry='geometry')\n",
        "    trg_poly_gdf = gpd.GeoDataFrame(trg_poly, geometry='geometry')\n",
        "    distance_series = src_poly_gdf.distance(trg_poly_gdf)\n",
        "    # print(distance_series)\n",
        "\n",
        "    # insert distance in merged data column\n",
        "    merged_data['v1'] = merged_data.source_X - merged_data.target_X\n",
        "    merged_data['v2'] = merged_data.source_Y - merged_data.target_Y\n",
        "    # merged_data['euc_distance'] = np.hypot(merged_data.v1, merged_data.v2)\n",
        "    merged_data['euc_distance'] = distance_series\n",
        "    # remove when distance \"illegal\"\n",
        "    valid_distance = merged_data['euc_distance'] < maximum_distance\n",
        "    not_same_node = merged_data['euc_distance'] != 0\n",
        "    data = merged_data[valid_distance & not_same_node]\n",
        "    # calculate azimuth\n",
        "    data['azimuth'] = np.degrees(np.arctan2(merged_data['v2'], merged_data['v1']))\n",
        "    data['bearing'] = (data.azimuth + 360) % 360\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8B7CEVY7tMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(number_of_scenarios, edges):\n",
        "  clean_up_file(\"*csv\")\n",
        "  clean_up_file(\"*png\")\n",
        "  scenarios_list = []\n",
        "  log_burned = []  # no removing duplicate\n",
        "  # --- SCENARIOS\n",
        "  t = datetime.datetime.now()\n",
        "  print(\"number of scenarios : {}\".format(number_of_scenarios))\n",
        "  for scenario in range(number_of_scenarios):\n",
        "      t0 = datetime.datetime.now()\n",
        "      burn_list = []\n",
        "      print(\"--- SCENARIO : {}\".format(scenario))\n",
        "      fire_list = set_initial_fire_to(edges)\n",
        "      if len(fire_list) == 0:\n",
        "          print(\"no fire\")\n",
        "          log_burned.extend(burn_list)\n",
        "          scenarios_list.extend([scenario] * len(burn_list))\n",
        "          continue\n",
        "      w_direction, w_speed, w_bearing = wind_scenario()\n",
        "      # --------- STEPS\n",
        "      for step in range(len(edges)):\n",
        "          print(\"--------- STEP : {}\".format(step))\n",
        "          fire_list = set_fire_to(edges, fire_list)\n",
        "          fire_list, burn_list = fire_spreading(fire_list, burn_list, w_speed, w_bearing, 0, step, edges)\n",
        "          if len(fire_list) == 0:\n",
        "              break\n",
        "      log_burned.extend(burn_list)\n",
        "      scenarios_list.extend([scenario] * len(burn_list))\n",
        "\n",
        "      # log_files_concatenate('step*', scenario)\n",
        "      \n",
        "  return scenarios_list, log_burned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7ZFBFEdij7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def postprocessing(scenarios_recorded, burned_asset, gdf_polygons):\n",
        "    list_of_tuples = list(zip(scenarios_recorded, burned_asset))\n",
        "    df = pd.DataFrame(list_of_tuples, columns=['scenarios', 'burned_asset_index'])\n",
        "    # df burn count per asset\n",
        "    df['count'] = df.groupby('burned_asset_index')['burned_asset_index'].transform('count')\n",
        "    print(df.describe())\n",
        "    df = df[['burned_asset_index', 'count']]\n",
        "    df_count = pd.merge(df, gdf_polygon, left_on='burned_asset_index', right_on='TARGET_FID', how='left')\n",
        "    # to geodataframe\n",
        "    crs = gdf_polygon.crs\n",
        "    gdf_count = gpd.GeoDataFrame(df_count, crs=crs, geometry='geometry')\n",
        "    # plot\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    gdf_count.plot(column='count', cmap='RdYlBu_r', ax=ax, legend=True)\n",
        "    ax.title.set_text(\"Burned buildings after {} scenarios\".format(max(scenarios_recorded)+1))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(path_output, \"results_{}.png\".format(max(scenarios_recorded)+1)))\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    gdf_count = gdf_count.drop(columns=['SHAPE_Leng', 'SHAPE_Area', 'AU2013Num', 'IgnProb_bl', 'RandProb'])\n",
        "    df_count.to_csv(os.path.join(path_output, \"results_{}_scenarios.csv\".format(max(scenarios_recorded)+1)))\n",
        "    gdf_count.to_file(os.path.join(path_output, \"results_{}_scenarios.shp\".format(max(scenarios_recorded)+1)))\n",
        "\n",
        "    \n",
        "    return df_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc9ZZUD_cepw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5db82351-6073-44d1-8595-1df5fb54cd57"
      },
      "source": [
        "%%time\n",
        "# load data\n",
        "gdf_polygon = load_data(\"buildings_raw.shp\", 1748570, 5425500, 1749500, 5427600) # small\n",
        "# gdf_polygon = load_data(\"buildings_raw.shp\", 1740508, 5420049, 1755776, 5443033) # whole\n",
        "geodataframe = gdf_polygon"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TARGET_FID       int64\n",
            "suburb_loc      object\n",
            "Combustibl       int64\n",
            "SHAPE_Leng     float64\n",
            "SHAPE_Area     float64\n",
            "AU2013Num        int64\n",
            "IgnProb_bl     float64\n",
            "RandProb       float64\n",
            "geometry      geometry\n",
            "dtype: object\n",
            "Shapefile extent : [1748498.0152998  5425264.79535007 1749563.95249987 5427653.74945021]\n",
            "Asset loaded : 1611\n",
            "Data size:0.00019663292914628983 GB'\n",
            "resized Data size:0.00015462283045053482 GB'\n",
            "CPU times: user 385 ms, sys: 41 ms, total: 426 ms\n",
            "Wall time: 432 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBN6MSTmUD-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b091f206-7b76-43af-be17-962b8141469d"
      },
      "source": [
        "%%time\n",
        "def edges_creation_Dask(geodataframe, nCores=os.cpu_count()):\n",
        "  # create list of sources & geometry\n",
        "  list_sourceID = np.repeat(np.array(geodataframe.TARGET_FID), len(geodataframe))\n",
        "  list_sourceID_geometry = np.repeat(np.array(geodataframe.geometry), len(geodataframe))\n",
        "\n",
        "  # create list of targets & geometry\n",
        "  list_targetID = [geodataframe.TARGET_FID] * len(geodataframe)\n",
        "  list_targetID = np.hstack(list_targetID)\n",
        "  list_targetID_geometry = [geodataframe.geometry] * len(geodataframe)\n",
        "  list_targetID_geometry = np.hstack(list_targetID_geometry)\n",
        "\n",
        "  # create Pandas dataframe\n",
        "  frame = { 'source': list_sourceID, 'source_geometry': list_sourceID_geometry, \n",
        "           'target': list_targetID, 'target_geometry': list_targetID_geometry } \n",
        "  df = pd.DataFrame(frame)\n",
        "\n",
        "  # create Dask dataframe\n",
        "  ddf = dd.from_pandas(df,npartitions=nCores)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9 µs, sys: 1 µs, total: 10 µs\n",
            "Wall time: 13.1 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNG_oIcMj_Pb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "25ffe970-f0e9-4845-8902-840d77980d3a"
      },
      "source": [
        "%%time\n",
        "edges = edges_creation_Dask(gdf_polygon)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 689 ms, sys: 7.32 ms, total: 696 ms\n",
            "Wall time: 681 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpisNkEWs9kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distance_calculation(edge_list):\n",
        "  \n",
        "  distance_series = edge_list[['source_geometry']].distance(edge_list[['target_geometry']])\n",
        "  return distance_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij7Pzu9St7Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "698d7333-f979-4e95-8647-51817a5e3541"
      },
      "source": [
        "d = distance_calculation(edges)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e50e6877db4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-9fd1c6811c97>\u001b[0m in \u001b[0;36mdistance_calculation\u001b[0;34m(edge_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdistance_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdistance_series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3439\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'DataFrame' object has no attribute %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'distance'"
          ]
        }
      ]
    }
  ]
}