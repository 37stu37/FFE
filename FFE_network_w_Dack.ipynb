{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzl21s2FyC7BUM9+Mc98gg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/FFE/blob/master/FFE_network_w_Dack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLEIMFiV55eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "%%capture\n",
        "!apt update\n",
        "!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal \n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree \n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0xvou966hTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0PZxCvmWCl",
        "colab_type": "code",
        "outputId": "d84c7532-d8f2-4deb-f259-69b99c556f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import networkx as nx\n",
        "from shapely.geometry import Point\n",
        "from sys import getsizeof\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 271 ms, sys: 55.8 ms, total: 327 ms\n",
            "Wall time: 947 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz4lyV9f3Sny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = Czlient(processes=False)\n",
        "client"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tLiFKrmZDM",
        "colab_type": "code",
        "outputId": "92c2a252-1191-4532-99ed-c2478a3a7cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input'\n",
        "path_output = '/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_output'\n",
        "\n",
        "!ls \"/content/drive/My Drive/05_Sync/FFE/FireNetwork/00_input\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buildings_raw.cpg  buildings_raw.prj  buildings_raw.shp  GD_wind.csv\n",
            "buildings_raw.dbf  buildings_raw.qpj  buildings_raw.shx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZosFepFncaX",
        "colab": {}
      },
      "source": [
        "def load_data(file_name, minx, miny, maxx, maxy):\n",
        "    # crop data\n",
        "    bbox = box(minx, miny, maxx, maxy)\n",
        "    # building point dataset\n",
        "    gdf_buildings = gpd.read_file(os.path.join(path, file_name), bbox=bbox)\n",
        "    print(gdf_buildings.dtypes)\n",
        "    max_extent = gdf_buildings.total_bounds\n",
        "    data_size = getsizeof(gdf_buildings)/(1024.0**3)\n",
        "    print(\"Shapefile extent : {}\".format(max_extent))\n",
        "    print(\"Asset loaded : {}\".format(len(gdf_buildings)))\n",
        "    print(\"Data size:{} GB'\".format(data_size))\n",
        "    # gdf_buildings.IgnProb_bl = 0.02\n",
        "    # xmin,ymin,xmax,ymax = gdf_buildings.total_bounds\n",
        "    # Precision of float32 is sufficient for lat and lon\n",
        "    float_columns = ['SHAPE_Leng','SHAPE_Area',\n",
        "                    'IgnProb_bl','RandProb']\n",
        "    gdf_buildings[float_columns] = gdf_buildings[float_columns].astype('float32')\n",
        "    int_columns = ['TARGET_FID','Combustibl',\n",
        "                    'AU2013Num','RandProb']\n",
        "    gdf_buildings[int_columns] = gdf_buildings[int_columns].astype('int32')\n",
        "    data_size = getsizeof(gdf_buildings)/(1024.0**3)\n",
        "    print(\"resized Data size:{} GB'\".format(data_size))\n",
        "    return gdf_buildings\n",
        "\n",
        "\n",
        "def wind_scenario():\n",
        "    wind_data = pd.read_csv(os.path.join(path, 'GD_wind.csv'))\n",
        "    i = np.random.randint(0, wind_data.shape[0])\n",
        "    w = wind_data.iloc[i, 2]\n",
        "    d = wind_data.iloc[i, 1]\n",
        "    b = wind_data.iloc[i, 3]\n",
        "    return w, d, b\n",
        "\n",
        "\n",
        "def create_network(edge_list_dataframe):\n",
        "    graph = nx.from_pandas_edgelist(edge_list_dataframe, edge_attr=True)\n",
        "    # options = {'node_color': 'red', 'node_size': 50, 'width': 1, 'alpha': 0.4,\n",
        "    #            'with_labels': False, 'font_weight': 'bold'}\n",
        "    # nx.draw_kamada_kawai(graph, **options)\n",
        "    # plt.show()\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV7sg0YI8V4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_edge_list_dack(geodataframe, maximum_distance, polygon_file):\n",
        "    # create arrays for different id combination\n",
        "    n = np.arange(0, len(geodataframe))\n",
        "    target = [n] * len(geodataframe)\n",
        "    target = np.hstack(target)\n",
        "    source = np.repeat(n, len(geodataframe))\n",
        "    # put arrays in dataframe\n",
        "    df = pd.DataFrame()\n",
        "    df['source_id'] = source\n",
        "    df['target_id'] = target\n",
        "    # merge source attributes with source index\n",
        "    geo_df = geodataframe.copy()\n",
        "    geo_df['id'] = geo_df.index\n",
        "    # create source / target gdf from gdf.columns of interest\n",
        "    geo_df = geo_df[['id', 'TARGET_FID', 'X', 'Y', 'geometry', 'IgnProb_bl']]\n",
        "    geo_df_TRG = geo_df.copy()\n",
        "    geo_df_TRG.columns = ['target_' + str(col) for col in geo_df_TRG.columns]\n",
        "    geo_df_SRC = geo_df.copy()\n",
        "    geo_df_SRC.columns = ['source_' + str(col) for col in geo_df_SRC.columns]\n",
        "    # merge data\n",
        "    merged_data = pd.merge(df, geo_df_SRC, left_on='source_id', right_on='source_id', how='outer')\n",
        "    merged_data = pd.merge(merged_data, geo_df_TRG, left_on='target_id', right_on='target_id', how='outer')\n",
        "    merged_data.rename(columns={'source_id': 'source', 'target_id': 'target'}, inplace=True)\n",
        "    # calculate distance for each source / target pair\n",
        "    # create a df from polygon shape to get accurate distance\n",
        "    # print(list(polygon_file))\n",
        "    polygon = polygon_file[['TARGET_FID', 'geometry']]\n",
        "    # print(list(polygon))\n",
        "    source_poly = merged_data[['source_TARGET_FID']]\n",
        "    target_poly = merged_data[['target_TARGET_FID']]\n",
        "    # print(list(source_poly))\n",
        "    src_poly = pd.merge(source_poly, polygon, left_on='source_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    trg_poly = pd.merge(target_poly, polygon, left_on='target_TARGET_FID', right_on='TARGET_FID', how='left')\n",
        "    src_poly_gdf = gpd.GeoDataFrame(src_poly, geometry='geometry')\n",
        "    trg_poly_gdf = gpd.GeoDataFrame(trg_poly, geometry='geometry')\n",
        "    distance_series = src_poly_gdf.distance(trg_poly_gdf)\n",
        "    # print(distance_series)\n",
        "\n",
        "    # insert distance in merged data column\n",
        "    merged_data['v1'] = merged_data.source_X - merged_data.target_X\n",
        "    merged_data['v2'] = merged_data.source_Y - merged_data.target_Y\n",
        "    # merged_data['euc_distance'] = np.hypot(merged_data.v1, merged_data.v2)\n",
        "    merged_data['euc_distance'] = distance_series\n",
        "    # remove when distance \"illegal\"\n",
        "    valid_distance = merged_data['euc_distance'] < maximum_distance\n",
        "    not_same_node = merged_data['euc_distance'] != 0\n",
        "    data = merged_data[valid_distance & not_same_node]\n",
        "    # calculate azimuth\n",
        "    data['azimuth'] = np.degrees(np.arctan2(merged_data['v2'], merged_data['v1']))\n",
        "    data['bearing'] = (data.azimuth + 360) % 360\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjmAYO9UnNOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_initial_fire_to(df):\n",
        "    \"\"\"Fine = 0, Fire = 1, Burned = 2\"\"\"\n",
        "    df['RNG'] = np.random.uniform(0, 1, size=len(df))  # add for random suppression per building, df.shape[0])\n",
        "    onFire = df['source_IgnProb_bl'] > df['RNG']\n",
        "    ignitions = df[onFire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(ignitions.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def set_fire_to(df, existing_fires):\n",
        "    are_set_on_fire = (df['source'].isin(existing_fires))\n",
        "    spark = df[are_set_on_fire]\n",
        "    # source nodes ignited\n",
        "    sources_on_fire = list(spark.source)\n",
        "    sources_on_fire = list(dict.fromkeys(sources_on_fire))\n",
        "    return sources_on_fire\n",
        "\n",
        "\n",
        "def fire_spreading(list_fires, list_burn, wind_speed, wind_bearing, suppression_threshold, step_value, data):\n",
        "    # check the fire potential targets\n",
        "    # print(\"fire list before spreading : {}, length : {}\".format(list_fires, len(list_fires)))\n",
        "    are_potential_targets = (data['source'].isin(list_fires))\n",
        "    are_not_already_burned = (~data['target'].isin(list_burn))\n",
        "    df = data[are_potential_targets & are_not_already_burned]\n",
        "    if df.empty:\n",
        "        # print(\"no fires\")\n",
        "        list_burn.extend(list(list_fires))\n",
        "        list_burn = list(dict.fromkeys(list_burn))\n",
        "        return [], list_burn  # to break the step loop\n",
        "    # set up additional CONDITIONS for fire spreading\n",
        "\n",
        "    # neighbors selection from buffer\n",
        "    are_neighbors = df['distance'] < wind_speed\n",
        "    df = df[are_neighbors]\n",
        "\n",
        "    # wind direction\n",
        "    wind_bearing_max = wind_bearing + 45\n",
        "    wind_bearing_min = wind_bearing - 45\n",
        "    if wind_bearing == 360:\n",
        "        wind_bearing_max = 45\n",
        "    if wind_bearing <= 0:  # should not be necessary\n",
        "        wind_bearing_min = 0\n",
        "    if wind_bearing == 999:\n",
        "        wind_bearing_max = 999\n",
        "        wind_bearing_min = 0\n",
        "    are_under_the_wind = (df['bearing'] < wind_bearing_max) & (df['bearing'] > wind_bearing_min)\n",
        "    # print(\"targets under the wind ? {}\".format(list(dict.fromkeys(list(are_under_the_wind)))))\n",
        "    df = df[are_under_the_wind]\n",
        "    # suppression\n",
        "    df['random'] = np.random.uniform(0, 1, size=len(df))\n",
        "    are_not_suppressed = df['random'] > suppression_threshold\n",
        "    # print(\"fire suppressed ? {}\".format(list(dict.fromkeys(list(are_not_suppressed)))))\n",
        "    df = df[are_not_suppressed]\n",
        "\n",
        "    # spread fire based on condition\n",
        "    fire_df = df\n",
        "    # fire_df = df[are_neighbors & are_under_the_wind & are_not_suppressed]  # issues with \"are_under_the_wind\n",
        "    # print(len(fire_df.head(5)))\n",
        "    # print(len(fire_df))\n",
        "    list_burn.extend(list(list_fires))\n",
        "    fire_df['step'] = step_value\n",
        "    # fire_df.to_csv(os.path.join(path_output, \"step{}_fire.csv\".format(step_value))) # ADD IF CSV OUTPUT NEEDED\n",
        "    list_fires = list(dict.fromkeys(list(fire_df.target)))\n",
        "    list_burn.extend(list(fire_df.target))\n",
        "    list_burn = list(dict.fromkeys(list_burn))\n",
        "    return list_fires, list_burn\n",
        "\n",
        "\n",
        "def log_files_concatenate(prefix, scenario_count):\n",
        "    list_df = []\n",
        "    files = glob.glob(os.path.join(path_output, prefix))\n",
        "    if files:\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(os.path.join(path_output, file))\n",
        "            list_df.append(df)\n",
        "            os.remove(file)\n",
        "        data = pd.concat(list_df)\n",
        "        data['scenario'] = scenario_count\n",
        "        data.to_csv(os.path.join(path_output, \"fire_scenario_{}.csv\".format(scenario_count)))\n",
        "    else:\n",
        "        print(\"no files to concatenate\")\n",
        "\n",
        "\n",
        "def clean_up_file(prefix, path_path=path_output):\n",
        "    files = glob.glob(os.path.join(path_path, prefix))\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        os.remove(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8B7CEVY7tMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(number_of_scenarios, edges):\n",
        "  clean_up_file(\"*csv\")\n",
        "  clean_up_file(\"*png\")\n",
        "  scenarios_list = []\n",
        "  log_burned = []  # no removing duplicate\n",
        "  # --- SCENARIOS\n",
        "  t = datetime.datetime.now()\n",
        "  print(\"number of scenarios : {}\".format(number_of_scenarios))\n",
        "  for scenario in range(number_of_scenarios):\n",
        "      t0 = datetime.datetime.now()\n",
        "      burn_list = []\n",
        "      print(\"--- SCENARIO : {}\".format(scenario))\n",
        "      fire_list = set_initial_fire_to(edges)\n",
        "      if len(fire_list) == 0:\n",
        "          print(\"no fire\")\n",
        "          log_burned.extend(burn_list)\n",
        "          scenarios_list.extend([scenario] * len(burn_list))\n",
        "          continue\n",
        "      w_direction, w_speed, w_bearing = wind_scenario()\n",
        "      # --------- STEPS\n",
        "      for step in range(len(edges)):\n",
        "          print(\"--------- STEP : {}\".format(step))\n",
        "          fire_list = set_fire_to(edges, fire_list)\n",
        "          fire_list, burn_list = fire_spreading(fire_list, burn_list, w_speed, w_bearing, 0, step, edges)\n",
        "          if len(fire_list) == 0:\n",
        "              break\n",
        "      log_burned.extend(burn_list)\n",
        "      scenarios_list.extend([scenario] * len(burn_list))\n",
        "\n",
        "      # log_files_concatenate('step*', scenario)\n",
        "      \n",
        "  return scenarios_list, log_burned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7ZFBFEdij7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def postprocessing(scenarios_recorded, burned_asset, gdf_polygons):\n",
        "    list_of_tuples = list(zip(scenarios_recorded, burned_asset))\n",
        "    df = pd.DataFrame(list_of_tuples, columns=['scenarios', 'burned_asset_index'])\n",
        "    # df burn count per asset\n",
        "    df['count'] = df.groupby('burned_asset_index')['burned_asset_index'].transform('count')\n",
        "    print(df.describe())\n",
        "    df = df[['burned_asset_index', 'count']]\n",
        "    df_count = pd.merge(df, gdf_polygon, left_on='burned_asset_index', right_on='TARGET_FID', how='left')\n",
        "    # to geodataframe\n",
        "    crs = gdf_polygon.crs\n",
        "    gdf_count = gpd.GeoDataFrame(df_count, crs=crs, geometry='geometry')\n",
        "    # plot\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    gdf_count.plot(column='count', cmap='RdYlBu_r', ax=ax, legend=True)\n",
        "    ax.title.set_text(\"Burned buildings after {} scenarios\".format(max(scenarios_recorded)+1))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(path_output, \"results_{}.png\".format(max(scenarios_recorded)+1)))\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    gdf_count = gdf_count.drop(columns=['SHAPE_Leng', 'SHAPE_Area', 'AU2013Num', 'IgnProb_bl', 'RandProb'])\n",
        "    df_count.to_csv(os.path.join(path_output, \"results_{}_scenarios.csv\".format(max(scenarios_recorded)+1)))\n",
        "    gdf_count.to_file(os.path.join(path_output, \"results_{}_scenarios.shp\".format(max(scenarios_recorded)+1)))\n",
        "\n",
        "    \n",
        "    return df_count"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}